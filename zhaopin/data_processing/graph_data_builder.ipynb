{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d219e-d7cb-41e5-88fb-fdffa259136e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json \n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch_geometric.data import Data, HeteroData, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch_geometric.nn import to_hetero\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b879fde-17b1-4e68-a631-bcf709d9f296",
   "metadata": {},
   "source": [
    "# Load sub-graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511d4f0-adb0-4151-986a-3168d52b7243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "truth_values = defaultdict(list)\n",
    "\n",
    "li = []\n",
    "\n",
    "# Retrieve ground truth values for each candidate-vacancy pair\n",
    "for truth in os.listdir(\"../ground_truth/\"):\n",
    "    if \".csv\" in truth:\n",
    "        df = pd.read_csv(f\"../ground_truth/{truth}\", header=None)\n",
    "        li.append(df)\n",
    "        \n",
    "        \n",
    "truths = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "truth_dict = {key1: dict(group[[1, 2]].values) for key1, group in truths.groupby(0)}\n",
    "truth_dict = {k: v for k, v in truth_dict.items() if not all([i <= 0 for i in v.values()])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30d444-37e6-4e69-8ec4-bb6af0bf62d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hits = defaultdict(lambda : defaultdict(lambda : defaultdict))\n",
    "misses = defaultdict(lambda : defaultdict(lambda : defaultdict))\n",
    "\n",
    "# Load data\n",
    "for i in os.listdir(\"../hits\"):\n",
    "    if \".json\" in i:\n",
    "        if i.split(\".\")[0] in truth_dict:\n",
    "            full = json.load(open(f\"../hits/{i}\"))\n",
    "            for k, v in full.items():\n",
    "                g = nx.node_link_graph(v)\n",
    "                hits[i.split(\".\")[0]][k] = g\n",
    "            \n",
    "                \n",
    "for i in os.listdir(\"../misses\"):\n",
    "    if \".json\" in i:\n",
    "        if i.split(\".\")[0] in truth_dict:\n",
    "            full = json.load(open(f\"../misses/{i}\"))\n",
    "            for k, v in full.items():\n",
    "                misses[i.split(\".\")[0]][k] = nx.node_link_graph(v)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df00e86-35c6-44bb-89bd-34baed453c36",
   "metadata": {},
   "source": [
    "# Create starting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3010ece-a7d5-45d2-993b-cd72f9483be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emb_size = 30\n",
    "\n",
    "label_types = {\"znp:supersedes\" : torch.rand(emb_size), \"znp:offers_function\" : torch.rand(emb_size), \"znp:has_worked_at\" : torch.rand(emb_size), \n",
    "               \"znp:falls_under\" : torch.rand(emb_size), \"znp:function_is_offered_by\" : torch.rand(emb_size),\"znp:has_employed\" : torch.rand(emb_size), \n",
    "               \"znp:subsedes\" : torch.rand(emb_size), \"znp:encompasses\" : torch.rand(emb_size), \"znp:browsed\" : torch.rand(emb_size), \n",
    "               \"znp:applied\" : torch.rand(emb_size), \"znp:fulfilled\" : torch.rand(emb_size), \"znp:lives_in\" : torch.rand(emb_size),\n",
    "               \"znp:wants_city\" : torch.rand(emb_size), \"znp:wants_industry\" : torch.rand(emb_size), \"znp:wants_job_type\" : torch.rand(emb_size),\n",
    "               \"znp:works_in_industry\" : torch.rand(emb_size), \"znp:works_in_job_type\" : torch.rand(emb_size), \"znp:has_degree\" : torch.rand(emb_size),\n",
    "               \"znp:has_min_salary\" : torch.rand(emb_size), \"znp:has_max_salary\" : torch.rand(emb_size), \"znp:wants_min_salary\" : torch.rand(emb_size), \n",
    "               \"znp:wants_max_salary\" : torch.rand(emb_size), \"znp:has_birthday\" : torch.rand(emb_size), \"znp:started_work_in\" : torch.rand(emb_size),\n",
    "               \"znp:is_stationed_in\" : torch.rand(emb_size), \"znp:is_job_type\" : torch.rand(emb_size), \"znp:requires_min_edu\" : torch.rand(emb_size),\n",
    "               \"znp:max_edu_level\" : torch.rand(emb_size), \"znp:offers_min_salary\" : torch.rand(emb_size), \"znp:offers_max_salary\" : torch.rand(emb_size),\n",
    "               \"znp:requires_travel\" : torch.rand(emb_size), \"znp:requires_years\" : torch.rand(emb_size),\n",
    "               \"rdf:type\": torch.rand(emb_size), \"dbo:education\": torch.rand(emb_size), \"owl:sameAs\": torch.rand(emb_size),\n",
    "               \"rnp:encompasses\": torch.rand(emb_size), \"rdfs:subClassOf\": torch.rand(emb_size), \"rdfs:comment\": torch.rand(emb_size)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3372e-1b3f-4230-8978-b71043c15977",
   "metadata": {},
   "source": [
    "# Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a1f82c-6d65-43f9-9a3a-ef9f1aa01cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hits = {user: {job: hits[user][job] for job in hits[user]} for user in hits}        \n",
    "\n",
    "misses = {user: {job: misses[user][job] for job in misses[user]} for user in misses}       \n",
    "\n",
    "hits_misses = {user: {**hits[user], **misses[user]} for user in {**hits, **misses}.keys() if (user in hits) and (user in misses)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287d752-0ed6-47a8-9106-f758856687d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_misses_filtered = {}\n",
    "\n",
    "# Some sub-graphs do not actually include the tail node (occurs when the only link to the tail is through the \"match\" edge, which is removed as that is the ground truth value of the graph\n",
    "# So, we ignore those as they are redundant\n",
    "for k, v in hits_misses.items():\n",
    "    hits_misses_filtered[k] = {tail: graph for tail, graph in v.items() if f\"zne:{tail}\" in graph and tail in truth_dict[k]}\n",
    "\n",
    "filtered_truth_dict = {k: {tail: truth_dict[k][tail] for tail in v if tail in truth_dict[k]} for k, v in hits_misses_filtered.items()}\n",
    " \n",
    "truth_dict = {k: v for k, v in filtered_truth_dict.items() if not all([i <= 0 for i in v.values()])}\n",
    "\n",
    "hits_misses_filtered = {k: v for k, v in hits_misses_filtered.items() if k in truth_dict}\n",
    "len(hits_misses_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86ba39-c719-48eb-8000-817e0bf7256f",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad89cf-fdea-4605-9c67-b9617e470f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(hits_misses, train_size=0.8, val_size=0.1, create_train = True, create_val = True, create_test = True, shuffle=False):\n",
    "\n",
    "    # Init\n",
    "    embedding_storage = defaultdict(lambda : defaultdict(lambda : defaultdict(torch.Tensor)))\n",
    "    \n",
    "    # Textual features (CVs and vacancies) should be tokenized so that they can be embedded later on\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-small\")\n",
    "\n",
    "    # Final data list\n",
    "    train_loader = []\n",
    "    val_loader = []\n",
    "    test_loader = []\n",
    "\n",
    "    # Ways to return nodes to their graphs and vice versa\n",
    "    graph_finder = {}\n",
    "    target_finder = {}\n",
    "\n",
    "    # Each node has a unique ID, this is to identify those\n",
    "    node_id_to_node = {}\n",
    "    node_to_node_id = {}\n",
    "\n",
    "    # Each graph in the dataset stored as a list of edges\n",
    "    og_graphs = {}\n",
    "\n",
    "    total_graphs = 0\n",
    "    nodes_per_graph = defaultdict(list)\n",
    "\n",
    "    # Start counting nodes at 1, since we have a single dummy node at index 0\n",
    "    total_nodes = 1\n",
    "    \n",
    "    train_cutoff = int(len(hits_misses) * train_size)\n",
    "    val_cutoff = int(len(hits_misses) * (train_size + val_size))\n",
    "\n",
    "    # Create a tensor for each user\n",
    "    for usr_idx, (user, graphs) in enumerate(hits_misses.items()):\n",
    "        \n",
    "        # Shuffle randomizes the samples in each loader\n",
    "        if shuffle:\n",
    "            bucket = np.random.random()\n",
    "\n",
    "            # Only create the requested loaders\n",
    "            if not create_train and (in_train := bucket < train_size):\n",
    "                continue\n",
    "            if not create_val and (in_val := train_size < bucket < (train_size + val_size)):\n",
    "                continue\n",
    "            if not create_test and (in_test := (train_size + val_size) < bucket):\n",
    "                continue\n",
    "        else: # Without shuffle, the samples in each loader are set\n",
    "            if not create_train and usr_idx < train_cutoff:\n",
    "                continue\n",
    "            if not create_val and usr_idx < val_cutoff:\n",
    "                continue\n",
    "            elif not create_test:\n",
    "                break\n",
    "\n",
    "\n",
    "        print(f\"{usr_idx}/{len(hits_misses)}\", end=\"\\r\")\n",
    "\n",
    "        user_graphs = []\n",
    "\n",
    "        temp = []\n",
    "        truth_values = []\n",
    "        heads = []\n",
    "        tails = []\n",
    "        targets = []\n",
    "        typing = {}\n",
    "\n",
    "        tail_nodes = []\n",
    "        head_nodes = []\n",
    "\n",
    "        node_graphs = {}\n",
    "        sg_counter = 0\n",
    "\n",
    "        edges_candidate_company = []\n",
    "        \n",
    "        for i, (tail, graph) in enumerate(graphs.items()):\n",
    "            \n",
    "            if graph:            \n",
    "                # Every node should be uniquely stored in each sub-graph, even if it's used in multiple graphs\n",
    "                graph = nx.relabel_nodes(graph, {node: f\"{node}_{total_graphs:08}\" for node in graph})\n",
    "\n",
    "                nx.set_edge_attributes(graph, {edge:i for edge in graph.edges()}, name=\"weight\")\n",
    "                relations = nx.get_edge_attributes(graph, \"edge_type\")\n",
    "\n",
    "                embs = {}\n",
    "                mask = {}\n",
    "\n",
    "                edge_embs = {}\n",
    "\n",
    "                # experiences = nx.get_node_attributes(graph, \"CV\")\n",
    "\n",
    "                # Store node embeddings\n",
    "                for node in graph:                \n",
    "                    if (\":\" in node) and (re.match(\"zne:j([0-9])+\", node) or re.match(\"zne:u([0-9])+\", node)):                               \n",
    "                        # If our current node is the head/tail of the sub-graph, store it for later use\n",
    "                        if node.split(\":\")[1].split(\"_\")[0] == tail:\n",
    "                            tail_nodes.append(total_nodes)\n",
    "                        elif node.split(\":\")[1].split(\"_\")[0] == user:\n",
    "                            head_nodes.append(total_nodes)\n",
    "\n",
    "                    # If we're dealing with a candidate\n",
    "                    if re.match(\"zne:u([0-9])+\", node):\n",
    "\n",
    "                        ### The textual features need to be locked in terms of size\n",
    "                        # CV = experiences[node].replace(\"|\", \" \") if type(experiences[node]) == str else \" \"\n",
    "                        CV_embedding_size = 96\n",
    "\n",
    "                        CV_encoded = tokenizer(\"query: \" + graph.nodes[node].get(\"CV\", \"\"), padding=\"max_length\", truncation=True, max_length=CV_embedding_size, return_tensors='pt')\n",
    "\n",
    "                        embs[node] = CV_encoded[\"input_ids\"]\n",
    "                        mask[node] = CV_encoded[\"attention_mask\"]\n",
    "\n",
    "                        typing[\"_\".join(node.split(\"_\")[:-1])] = \"candidate\"\n",
    "\n",
    "                    elif re.match(\"zne:j([0-9])+\", node):\n",
    "\n",
    "                        vacancy_embedding_size = 96\n",
    "\n",
    "                        vacancy_encoded = tokenizer(\"passage: \" + graph.nodes[node].get(\"vacancy\", \"\"), padding=\"max_length\", truncation=True, max_length=vacancy_embedding_size, return_tensors='pt')\n",
    "\n",
    "                        embs[node] = vacancy_encoded[\"input_ids\"]\n",
    "                        mask[node] = vacancy_encoded[\"attention_mask\"]\n",
    "\n",
    "                        typing[\"_\".join(node.split(\"_\")[:-1])] = \"vacancy\"\n",
    "\n",
    "                    else:\n",
    "                        # Any other node type simply starts with a random embedding\n",
    "                        random_embedding_size = 16\n",
    "\n",
    "                        embs[node] = torch.rand(random_embedding_size)\n",
    "\n",
    "\n",
    "                        if node.startswith(\"zne:city_\"):\n",
    "                            typing[\"_\".join(node.split(\"_\")[:-1])] = \"city\"\n",
    "                        elif node.startswith(\"zne:edu\"):\n",
    "                            typing[\"_\".join(node.split(\"_\")[:-1])] = \"education\"\n",
    "                        elif node.startswith(\"zne:job_type_\"):\n",
    "                            typing[\"_\".join(node.split(\"_\")[:-1])] = \"job_type\"\n",
    "                        elif node.startswith(\"zne:industry_\"):\n",
    "                            typing[\"_\".join(node.split(\"_\")[:-1])] = \"industry\"\n",
    "                        elif node.startswith(\"znd:\") or node.startswith(\"owl:\") or node.startswith(\"foaf:\"): \n",
    "                            typing[\"_\".join(node.split(\"_\")[:-1])] = \"klass\"\n",
    "                        else:\n",
    "                            typing[\"_\".join(node.split(\"_\")[:-1])] = \"literal\"\n",
    "\n",
    "                    # Give each node a fully unique ID\n",
    "                    node_id_to_node[total_nodes] = node\n",
    "                    node_to_node_id[node] = total_nodes\n",
    "\n",
    "\n",
    "                    # Store to which graph that ID belongs\n",
    "                    node_graphs[node] = sg_counter\n",
    "\n",
    "                    # nodes_per_graph[total_graphs].append(total_nodes)\n",
    "\n",
    "                    total_nodes += 1\n",
    "\n",
    "\n",
    "                # Store edge embeddings\n",
    "                for edge in graph.edges():\n",
    "                    edge_embs[edge] = label_types[relations[edge]]\n",
    "\n",
    "                # Add embeddings to graph\n",
    "                nx.set_node_attributes(graph, embs, name=\"embedding\")\n",
    "                nx.set_node_attributes(graph, mask, name=\"att_mask\")\n",
    "\n",
    "\n",
    "\n",
    "                # Store information (head, tail, and structure) of current graph based on its id\n",
    "                target_finder[total_graphs] = (user, tail)\n",
    "                og_graphs[total_graphs] = [(edge[0].split(\"_\")[0], edge[1].split(\"_\")[0]) for edge in graph.edges()]\n",
    "\n",
    "\n",
    "                nx.set_edge_attributes(graph, {edge: total_graphs for edge in graph.edges()}, \n",
    "                                       name=\"sub_graph\")\n",
    "\n",
    "                # Store which sub-graph IDs relate to what candidate\n",
    "                user_graphs.append(total_graphs)\n",
    "\n",
    "                total_graphs += 1\n",
    "\n",
    "                nx.set_edge_attributes(graph, edge_embs, name=\"embedding\")\n",
    "\n",
    "\n",
    "                temp.append(graph)\n",
    "                      \n",
    "                if user in truth_dict and tail in truth_dict[user]:\n",
    "                    truth_values.append(truth_dict[f\"{user}\"][tail])\n",
    "                    heads.append(f\"{user}\")\n",
    "                    tails.append(f\"{tail}\")\n",
    "                else:\n",
    "                    truth_values.append(0)\n",
    "\n",
    "                sg_counter += 1\n",
    "\n",
    "        # Ground truths\n",
    "        y = torch.Tensor([i if i >= 0 else 0 for i in truth_values])  \n",
    "\n",
    "        # If we have no sorting to do, ignore the current batch\n",
    "        if all(y == 0):\n",
    "            print(\"skipping\")\n",
    "            continue\n",
    "\n",
    "        # Combine sub-graphs\n",
    "        G = nx.compose_all(temp)\n",
    "\n",
    "        data = HeteroData()\n",
    "\n",
    "        data_dict = defaultdict(list)\n",
    "        mask_dict = defaultdict(list)\n",
    "        sg_dict = defaultdict(list)\n",
    "\n",
    "        index_dict = defaultdict(list)\n",
    "        unique_index_dict = defaultdict(list)\n",
    "\n",
    "        typings = [\"city\", \"education\", \"job_type\", \"industry\", \"klass\", \"literal\"]\n",
    "\n",
    "        dummy_node_id = 0\n",
    "\n",
    "        # Store node data in the appropriate data lists\n",
    "        for k, v in G.nodes(data=True):\n",
    "\n",
    "            node, node_id = \"_\".join(k.split(\"_\")[:-1]), k.split(\"_\")[-1]\n",
    "\n",
    "\n",
    "            data_dict[typing[node]].append(v[\"embedding\"])\n",
    "\n",
    "            if \"att_mask\" in v:\n",
    "                mask_dict[typing[node]].append(v[\"att_mask\"])\n",
    "\n",
    "            sg_dict[typing[node]].append(node_graphs[k])\n",
    "\n",
    "            index_dict[typing[node]].append(node_id)\n",
    "            unique_index_dict[typing[node]].append(node_to_node_id[k])\n",
    "\n",
    "        for node_type in data_dict.keys():\n",
    "            data[node_type].node_id = torch.Tensor([int(i) for i in index_dict[node_type]])\n",
    "            data[node_type].unique_node_id = torch.Tensor([int(i) for i in unique_index_dict[node_type]])\n",
    "            data[node_type].sub_graph = torch.Tensor([int(i) for i in sg_dict[node_type]])\n",
    "            data[node_type].x = torch.stack(data_dict[node_type]).squeeze(1)\n",
    "            data[node_type].num_nodes = len(index_dict[node_type])\n",
    "\n",
    "            if node_type in [\"candidate\", \"vacancy\"]:\n",
    "                data[node_type].att_mask = torch.stack(mask_dict[node_type]).squeeze(1)\n",
    "\n",
    "        for node_type in typings:\n",
    "            if node_type not in data_dict:\n",
    "                data[node_type].node_id = torch.Tensor([dummy_node_id])\n",
    "                data[node_type].unique_node_id = torch.Tensor([dummy_node_id])\n",
    "                data[node_type].x = torch.zeros(random_embedding_size)\n",
    "                data[node_type].num_nodes = 1\n",
    "\n",
    "                if node_type in [\"candidate\", \"vacancy\"]:\n",
    "                    data[node_type].att_mask = torch.zeros(random_embedding_size) + 1\n",
    "\n",
    "        # Now do the same for the edges\n",
    "        edge_data = defaultdict(list)\n",
    "        edge_ids = defaultdict(list)\n",
    "\n",
    "        H = G.copy()\n",
    "        H = nx.relabel_nodes(H, {node: int(node.split(\"_\")[-1]) for node in H})\n",
    "\n",
    "        sub_graphs = nx.get_edge_attributes(H, \"sub_graph\")\n",
    "        graph_finder = {**graph_finder, **sub_graphs}\n",
    "\n",
    "        typings = [\"candidate\", \"vacancy\", \"city\", \"education\", \"job_type\", \"industry\", \"klass\", \"literal\"]\n",
    "\n",
    "        # Define all the possible edge types    \n",
    "        edge_typing = {(\"candidate\", \"vacancy\") : \"fulfilled\",\n",
    "                       (\"vacancy\", \"candidate\") : \"was_fulfilled_by\",\n",
    "                       (\"candidate\", \"education\") : \"has_education\",\n",
    "                       (\"education\", \"candidate\") : \"is_education_of\",\n",
    "                       (\"candidate\", \"city\") : \"lives_in_city\",\n",
    "                       (\"city\", \"candidate\") : \"is_city_of_candidate\",\n",
    "                       (\"candidate\", \"job_type\") : \"works_in_field\",\n",
    "                       (\"job_type\", \"candidate\") : \"is_field_of_candidate\",\n",
    "                       (\"candidate\", \"industry\") : \"works_in_industry\",\n",
    "                       (\"industry\", \"candidate\") : \"is_industry_of\",\n",
    "\n",
    "                       (\"vacancy\", \"job_type\") : \"vacancy_is_of_type\",\n",
    "                       (\"job_type\", \"vacancy\") : \"is_type_of_vacancy\", \n",
    "                       \n",
    "                       (\"vacancy\", \"industry\") : \"vacancy_is_in_industry\",\n",
    "                       (\"industry\", \"vacancy\") : \"is_industry_of_vacancy\", \n",
    "                       \n",
    "                       (\"vacancy\", \"city\") : \"vacancy_is_in_city\",\n",
    "                       (\"city\", \"vacancy\") : \"is_city_of_vacancy\", \n",
    "                       \n",
    "                       (\"vacancy\", \"education\") : \"vacancy_requires_education\",\n",
    "                       (\"education\", \"vacancy\") : \"is_required_education_of\"}\n",
    "\n",
    "        # Add a few iteratively since they're the same for everything\n",
    "        for typ in set(typings):\n",
    "            edge_typing[(typ, \"klass\")] =  f\"{typ}_is_part_of_class\"\n",
    "            edge_typing[(\"klass\", typ)] =  f\"{typ}_is_class_of\"\n",
    "            edge_typing[(typ, \"literal\")] = f\"{typ}_has_literal_value\"\n",
    "            edge_typing[(\"literal\", typ)] = f\"{typ}_is_literal_value_of\"      \n",
    "            edge_typing[(typ, typ)] = f\"{typ}_same_as\"\n",
    "\n",
    "        for i, j in G.edges():\n",
    "\n",
    "            edge1, edge1_id = \"_\".join(i.split(\"_\")[:-1]), i.split(\"_\")[-1]\n",
    "            edge2, edge2_id = \"_\".join(j.split(\"_\")[:-1]), j.split(\"_\")[-1]\n",
    "\n",
    "            type1, type2 = typing[edge1], typing[edge2]\n",
    "\n",
    "            edge_data[edge_typing[(type1, type2)]].append((index_dict[type1].index(edge1_id), index_dict[type2].index(edge2_id)))\n",
    "            edge_ids[edge_typing[(type1, type2)]].append((int(edge1_id), int(edge2_id)))\n",
    "\n",
    "\n",
    "        dummy_embedding = torch.zeros_like(next(iter(nx.get_node_attributes(G, \"embedding\").values())))  # Assuming all node embeddings have the same shape\n",
    "        G.add_node(dummy_node_id, embedding=dummy_embedding)\n",
    "\n",
    "        for k, v in edge_typing.items():\n",
    "            if edge_data[v]:\n",
    "                data[k[0], v, k[1]].edge_index = torch.LongTensor(edge_data[v]).T\n",
    "                data[k[0], v, k[1]].edge_id = torch.LongTensor(edge_ids[v]).T\n",
    "            else:\n",
    "                dummy_edge_index = [dummy_node_id, dummy_node_id]\n",
    "\n",
    "                data[k[0], v, k[1]].edge_index = torch.LongTensor([dummy_edge_index]).T\n",
    "                data[k[0], v, k[1]].edge_id = torch.LongTensor([dummy_node_id]).T\n",
    "\n",
    "        node_order = list(G.nodes())\n",
    "\n",
    "        data.head_nodes = head_nodes\n",
    "        data.tail_nodes = tail_nodes\n",
    "\n",
    "        data.num_graphs = torch.LongTensor(user_graphs)\n",
    "        data.y = y\n",
    "        data.tups = list(zip(heads, tails))\n",
    "\n",
    "        if shuffle: # With shuffle, the sample's loader has been determined by its bucket earlier\n",
    "            if in_train:\n",
    "                train_loader.append(data)\n",
    "            elif in_val:\n",
    "                val_loader.append(data)\n",
    "            else:\n",
    "                test_loader.append(data)\n",
    "        else:\n",
    "            if usr_idx < train_cutoff:\n",
    "                train_loader.append(data)\n",
    "            elif usr_idx < val_cutoff:\n",
    "                val_loader.append(data)\n",
    "            else:\n",
    "                test_loader.append(data)\n",
    "            \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e573692-2bc5-42af-9975-882e4bd956f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/load dataloaders\n",
    "train_loader, val_loader, test_loader = create_dataloaders(hits_misses_filtered,  \n",
    "                                                           create_train = create_train, \n",
    "                                                           create_val = create_val, \n",
    "                                                           create_test = create_test)\n",
    "\n",
    "### Batching not possible due to the fact that different graphs have different numbers of edge for each type - Considering the RAM usage, we did not implement this with, e.g., padding. \n",
    "trainloader = DataLoader(train_loader) \n",
    "valloader = DataLoader(val_loader) \n",
    "testloader = DataLoader(test_loader)\n",
    "\n",
    "torch.save(trainloader, '../dataloaders/graph_trainloader.pth')\n",
    "torch.save(valloader, '../dataloaders/graph_valloader.pth')\n",
    "torch.save(testloader, '../dataloaders/graph_testloader.pth')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f2cf1-bc22-4383-a875-f2cde3b4c8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
